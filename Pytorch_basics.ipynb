{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1I6TFru2EIX",
        "outputId": "1643f7a5-5f68-4784-9d02-14f02e523bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Value : 12.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([2.0])\n",
        "y = x**3 + 5\n",
        "\n",
        "grad_x = 3 * (x**2)\n",
        "\n",
        "print(f\"Gradient Value : {grad_x[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lylerpci3fJ_",
        "outputId": "ecf00be0-13e2-4115-df5b-2d8c83f1f07b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Value: 12.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([2.0], requires_grad = True)\n",
        "\n",
        "y = x**3 + 5\n",
        "\n",
        "y.backward()\n",
        "\n",
        "print(f\"Gradient Value: {x.grad.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK0YA8ng4H57",
        "outputId": "9c86a1eb-eb76-4e42-c001-efded1f679fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight Shape: torch.Size([5, 10])\n",
            "Bias Shape: torch.Size([5])\n",
            "tensor([[ 0.3628, -1.0431,  0.4777, -0.8565,  0.0311]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "layer = nn.Linear(in_features = 10, out_features = 5)\n",
        "\n",
        "print(f\"Weight Shape: {layer.weight.shape}\")\n",
        "print(f\"Bias Shape: {layer.bias.shape}\")\n",
        "\n",
        "# Automatically assigns Weights and bias respective to input features and hidden layer neuron counts\n",
        "x = torch.randn(1,10)\n",
        "\n",
        "output = layer(x)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdJYBiMl58_3",
        "outputId": "6ca6d264-92e3-4b5c-c0a3-96fb7c34c459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.3739,  1.3974,  0.0834, -0.4271, -1.0409, -0.4697,  1.4135, -0.5688,\n",
            "          0.4498,  0.0772]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3739, 1.3974, 0.0834, 0.0000, 0.0000, 0.0000, 1.4135, 0.0000, 0.4498,\n",
              "         0.0772]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "print(x)\n",
        "x.relu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hevvaRzZ7ErV"
      },
      "source": [
        "**Out of Place Memory Operation vs In Place Memory Operation**\n",
        "\n",
        "    *By adding _(Underscore) after a function we can make the same function work as In place operation*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hSv5P6s7EWA",
        "outputId": "861a2549-1f95-4c17-e8bb-fd704853b6ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out of Place Calculation...\n",
            "Original A Memory Address: 139987236432272\n",
            "Original A Values: tensor([1., 2., 3.])\n",
            "New B memory address:139987237401488\n",
            "Values of B: tensor([ 2.7183,  7.3891, 20.0855])\n",
            "Values of A (unchanged):   tensor([1., 2., 3.])\n"
          ]
        }
      ],
      "source": [
        "print(\"Out of Place Calculation...\")\n",
        "\n",
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "\n",
        "print(f\"Original A Memory Address: {id(a)}\")\n",
        "print(f\"Original A Values: {a}\")\n",
        "\n",
        "\n",
        "b = torch.exp(a)\n",
        "print(f\"New B memory address:{id(b)}\")\n",
        "print(f\"Values of B: {b}\")\n",
        "print(f\"Values of A (unchanged):   {a}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHlb1-mt8Lqn",
        "outputId": "52733b04-a55a-4555-dd1e-be5d20e8d21b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Place Calculation...\n",
            "Original X Memory Address: 139987636180928\n",
            "Original X Values: tensor([1., 2., 3.])\n",
            "New Y memory address:139987636180928\n",
            "Values of Y: tensor([ 2.7183,  7.3891, 20.0855])\n",
            "Values of X (changed): tensor([ 2.7183,  7.3891, 20.0855])\n"
          ]
        }
      ],
      "source": [
        "print(\"In Place Calculation...\")\n",
        "\n",
        "X = torch.tensor([1.0, 2.0, 3.0])\n",
        "\n",
        "print(f\"Original X Memory Address: {id(X)}\")\n",
        "print(f\"Original X Values: {X}\")\n",
        "\n",
        "\n",
        "y = torch.exp_(X)\n",
        "print(f\"New Y memory address:{id(y)}\")\n",
        "print(f\"Values of Y: {y}\")\n",
        "print(f\"Values of X (changed): {X}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO6PBr_M8otF",
        "outputId": "42385857-a4c8-4450-d33f-85d298343a53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caught Expected Error:\n",
            "a leaf Variable that requires grad is being used in an in-place operation.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "try:\n",
        "    y = x.pow_(2)\n",
        "    y.backward()\n",
        "except RuntimeError as e:\n",
        "    print(f\"Caught Expected Error:\\n{e}\")\n",
        "\n",
        "# This is using in place calculation  while autograd where it gives error due to change in the function value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um3loCACVBAT",
        "outputId": "12889552-3232-43aa-c8d3-1c097fa56263"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "device\n",
        "# it will assign cuda if there is GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuADda3mVcz8",
        "outputId": "4836972b-c8c6-476f-f97e-f4698ebe7490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0], device = device)\n",
        "print(x.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "P74f0n9nWJdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "932711c6-c471-4343-f010-554d69e61bfd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "cpu_tensor = torch.tensor([10.0, 25.0, 20.0])\n",
        "cpu_tensor.device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_tensor = cpu_tensor.to(device)\n",
        "gpu_tensor.device\n",
        "\n",
        "# changing to gpu using to function which helps in convert data to gpu after loading the data\n",
        "# We need to assign the changed variable else it will stay in CPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZaStBarYoMK",
        "outputId": "92c4b821-622d-4c99-d8b7-a3309709f809"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([2.0], requires_grad = True)\n",
        "print(x.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8-DVxonY_CH",
        "outputId": "94f9155a-b4e0-4e11-d976-fbcf89f31887"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  y = x*3\n",
        "  print(f\"Y requires grad:{y.requires_grad}\")\n",
        "  print(f\"Y grad Function: {y.grad_fn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6dKMC0Lc10m",
        "outputId": "b0366418-e145-4414-be08-0482b52facde"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y requires grad:False\n",
            "Y grad Function: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = x * 2\n",
        "print(f\"Outside no_grad: {z.requires_grad}\")\n",
        "\n",
        "# If we perform operations outside the no_grad the grad computations all be calculated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVXDw8sB22w4",
        "outputId": "512848e3-228d-4f29-9eee-26f539d9431c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outside no_grad: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "\n",
        "  y = x ** 2\n",
        "  print(f\"Inside Inference_node: {y.requires_grad}\")\n",
        "\n",
        "  # By using inference_mode we completely cutoff the tracking such as version and view tracking by removing extra logs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71drzGMX3JUf",
        "outputId": "58505446-3990-49f9-960d-d8eb64d9dc64"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside Inference_node: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.tensor([10.0], requires_grad=True)\n",
        "loss = w * 5\n",
        "\n",
        "detached_loss = loss.detach()\n",
        "\n",
        "print(f\"Original Loss: {loss.requires_grad}\")\n",
        "print(f\"Detached Loss: {detached_loss.requires_grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_yuxIo23w7g",
        "outputId": "2e3d5aaf-3207-448a-8b44-8fa608b119ae"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Loss: True\n",
            "Detached Loss: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detached_loss.add_(1)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSIK6aB86QOV",
        "outputId": "09ce10f0-304f-432b-d865-e2ea71952308"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([51.], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "y = x**2\n",
        "\n",
        "z = y**3\n",
        "\n",
        "z.backward()\n",
        "\n",
        "print(f\"Gradient dx: {x.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MwKbG5G6VrL",
        "outputId": "9883e7c8-60ae-4f38-e1e5-b1d2097cbe58"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient dx: tensor([192.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "y = x**2\n",
        "\n",
        "# y_detached has the same value as y (4.0) but NO relation to x.\n",
        "y_detached = y.detach()\n",
        "\n",
        "z = y_detached**3\n",
        "\n",
        "z.backward()\n",
        "\n",
        "print(f\"Gradient dx: {x.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "VP4y_4tz68iP",
        "outputId": "9dc68700-d604-45cf-c7d6-67c3017405f1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2332688083.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_detached\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Gradient dx: {x.grad}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "a = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "\n",
        "b = a.detach()\n",
        "\n",
        "print(f\"a before: {a}\")\n",
        "print(f\"b before: {b}\")\n",
        "\n",
        "b.zero_() # In place operation will change the original also\n",
        "\n",
        "print(f\"b after change: {b}\")\n",
        "print(f\"a after change: {a}\")\n",
        "\n",
        "# The detach function will be usefull when we need a part of gradient step to calculate some function without altering the gradient or computation graphs"
      ],
      "metadata": {
        "id": "nBkLW36y7bKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "972c260a-64df-433c-aaea-656a18029320"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a before: tensor([1., 2., 3.], requires_grad=True)\n",
            "b before: tensor([1., 2., 3.])\n",
            "b after change: tensor([0., 0., 0.])\n",
            "a after change: tensor([0., 0., 0.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.tensor(2.0, requires_grad=True)\n",
        "z = t.cos().exp_()\n",
        "z.backward()\n",
        "z"
      ],
      "metadata": {
        "id": "bzHQD9Fm74om",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "390c996e-d79e-4fcc-c24a-0a22e2bd3ec6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6596, grad_fn=<ExpBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.tensor(2.0, requires_grad=True)\n",
        "z = t.cos_().exp()\n",
        "z.backward()\n",
        "\n",
        "# we changed the input tensor by calculating in memory operation using directly with t.cos_() so it will cause error"
      ],
      "metadata": {
        "id": "ciW5v-H89YLB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "6a4009e1-8840-480f-bc2a-ddadce4b23b9"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-279763007.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# we changed the input tensor by calculating in memory operation using directly with t.cos_() so it will cause error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.tensor(2.0, requires_grad=True)\n",
        "z = t.exp().cos_()\n",
        "z.backward()\n",
        "\n",
        "# any function that changes the input values which required during back propagation of chain rule should not be performed using in place operations"
      ],
      "metadata": {
        "id": "_jmPdXoG-FC8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "d8338550-dcf7-4856-ee98-730f1249ee2b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of ExpBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1099364081.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# any function that changes the input values which required during back propagation of chain rule should not be performed using in place operations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of ExpBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# In_features = 100, Out_features = 200\n",
        "layer = nn.Linear(in_features=100, out_features=200)\n",
        "\n",
        "print(f\"Weight Shape: {layer.weight.shape}\")\n",
        "\n",
        "print(f\"Bias Shape:   {layer.bias.shape}\")"
      ],
      "metadata": {
        "id": "v1gXdqFPA8Y0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4adedf6-7b5e-4c12-db6f-4235ea9bce5c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight Shape: torch.Size([200, 100])\n",
            "Bias Shape:   torch.Size([200])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_true = torch.tensor([[12.0], [14.0], [16.0], [18.0]])\n",
        "\n",
        "model = nn.Linear(1, 1)\n",
        "\n",
        "criterion = nn.MSELoss() # Mean Squared Error\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01) # Stochastic Gradient Descent\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "print(f\"Initial params: Weight={model.weight.item():.2f}, Bias={model.bias.item():.2f}\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    #Set Mode\n",
        "    model.train()\n",
        "\n",
        "    # Forward Pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # Compute Loss\n",
        "    loss = criterion(y_pred, y_true)\n",
        "\n",
        "    # We clear previous gradients BEFORE calculating new ones\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "print(f\"Final params: Weight={model.weight.item():.2f}, Bias={model.bias.item():.2f}\")\n",
        "\n",
        "\n",
        "# A simple feed forward neural network with backpropagation\n"
      ],
      "metadata": {
        "id": "TwoIF_GRBmVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c9198d9-5e7e-4cab-c3e2-801179f35a06"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial params: Weight=-0.95, Bias=0.34\n",
            "Epoch 20: Loss = 9.2603\n",
            "Epoch 40: Loss = 7.9649\n",
            "Epoch 60: Loss = 7.0645\n",
            "Epoch 80: Loss = 6.2661\n",
            "Epoch 100: Loss = 5.5579\n",
            "Final params: Weight=3.96, Bias=4.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "count_cpu = os.cpu_count()\n",
        "optimal_workers = min(count_cpu, 16)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset = demo,\n",
        "    batch_size = 64,\n",
        "    shuffle = True,\n",
        "    num_workers = optimal_workers,   # create multiple process to handle multiple tasks such as text processing and data loading at the same time\n",
        "    persistent_workers = True,       # The memory allocated for each epoch didnt kill and ready to process the next epoch\n",
        "    pin_memory = True,                # Fast RAM to VRAM transfer as it always stores the data batches in cpu fixed memory and ready to load\n",
        "    prefetch_factor=2                  # This assigns two batches per worker\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# how to handle data loading with efficient and fast loading time to GPU"
      ],
      "metadata": {
        "id": "Pk6hRn2uG0Mh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "f3a4c473-6093-406d-ad40-90d7d4ed5124"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'demo' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-165806849.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m train_loader = DataLoader(\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'demo' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "model = nn.Linear(10,1)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "input_data = torch.randn(32, 10)\n",
        "target = torch.empty(32, 1).random_(2)\n",
        "\n",
        "logits = model(input_data)\n",
        "loss = criterion(logits, target)\n",
        "\n",
        "print(loss)\n",
        "\n",
        "\n",
        "# Used for binary class classification to calculate directly from logits then it also apply by default sigmoid while using BCEwith LogitLoss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN5s4SeIy00C",
        "outputId": "dbb9915e-1a06-488d-c93f-f7c91b9fd424"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8415, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Linear(10, 3)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "input_data = torch.randn(5, 10)\n",
        "target = torch.tensor([0, 2, 1, 0, 2], dtype=torch.long)\n",
        "\n",
        "logits = model(input_data)\n",
        "loss = criterion(logits, target)\n",
        "loss\n",
        "\n",
        "# This does Multiclassclassification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMqb650k2j1V",
        "outputId": "6c7ef0e6-07a0-4835-85d5-8a34b3f2c226"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.2440, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SensitivityNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Dropout: 50% chance to zero out a value\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        # BatchNorm: Normalizes data\n",
        "        self.bn = nn.BatchNorm1d(num_features=1)\n",
        "        self.linear = nn.Linear(1, 1, bias=False)\n",
        "\n",
        "        # Manually setting weight to 1.0\n",
        "        with torch.no_grad():\n",
        "            self.linear.weight.fill_(1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "model = SensitivityNet()\n",
        "input_data = torch.tensor([[10.0], [10.0]]) # Batch of two identical inputs\n",
        "\n",
        "print(\"--- TRAINING MODE ---\")\n",
        "model.train() # SWITCH Training mode\n",
        "output_train = model(input_data)\n",
        "# Dropout is ACTIVE: Some values might be 0.\n",
        "# BatchNorm is using THIS BATCH to calculate mean/std.\n",
        "\n",
        "print(output_train)\n",
        "\n",
        "print(\"\\n--- EVALUATION MODE ---\")\n",
        "model.eval() # SWITCH Evaluation mode\n",
        "output_eval = model(input_data)\n",
        "\n",
        "print(output_eval)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xc3zvqOi7lW8",
        "outputId": "6c57b8e7-818d-4b52-a110-89c40c0ff6b7"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TRAINING MODE ---\n",
            "tensor([[-0.0001],\n",
            "        [ 0.0000]], grad_fn=<MmBackward0>)\n",
            "\n",
            "--- EVALUATION MODE ---\n",
            "tensor([[9.4868],\n",
            "        [9.4868]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def logic_gate(x):\n",
        "    if x.sum() > 0:\n",
        "        return x\n",
        "    else:\n",
        "        return -x\n",
        "\n",
        "dummy_input = torch.tensor([10.0])\n",
        "\n",
        "# It records ONLY: \"return x\". It ignores the \"else\" block completely.\n",
        "traced_func = torch.jit.trace(logic_gate, dummy_input)\n",
        "\n",
        "test_input = torch.tensor([-10.0])\n",
        "\n",
        "print(f\"Traced Output: {traced_func(test_input)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80nu8S8d5jPD",
        "outputId": "c8f74c29-2007-4478-fc0f-e3d0af27646b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traced Output: tensor([-10.])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1349842164.py:2: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if x.sum() > 0:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The compiler reads the source code. It sees the \"if\". It preserves the logic.\n",
        "scripted_func = torch.jit.script(logic_gate)\n",
        "\n",
        "test_input = torch.tensor([-10.0])\n",
        "\n",
        "print(f\"Scripted Output: {scripted_func(test_input)}\")\n",
        "\n",
        "print(scripted_func.code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqgOaJ8zRH3m",
        "outputId": "e2a05d67-2405-476c-ccc4-c36784e95afb"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scripted Output: tensor([10.])\n",
            "def logic_gate(x: Tensor) -> Tensor:\n",
            "  if bool(torch.gt(torch.sum(x), 0)):\n",
            "    _0 = x\n",
            "  else:\n",
            "    _0 = torch.neg(x)\n",
            "  return _0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(1.2, requires_grad=True)\n",
        "y = torch.tensor(3.4, requires_grad=True)\n",
        "\n",
        "# PyTorch builds the computational graph dynamically here\n",
        "f = torch.sin((x**2) * y)\n",
        "f.backward()\n",
        "\n",
        "dx = x.grad\n",
        "dy = y.grad\n",
        "\n",
        "print(f\"Function value f(1.2, 3.4): {f.item():.4f}\")\n",
        "print(f\"Gradient vector [df/dx, df/dy]: [{dx.item():.4f}, {dy.item():.4f}]\")"
      ],
      "metadata": {
        "id": "v_k5m6AeRcQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3072e20-2ddc-4a6b-f27e-3c1e5e84000c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function value f(1.2, 3.4): -0.9832\n",
            "Gradient vector [df/dx, df/dy]: [1.4899, 0.2629]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nwlULOAFSZt8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}